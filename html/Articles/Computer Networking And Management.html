<!DOCTYPE html>
<html lang="en">
  <head>
    <title>CNM</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css"
      
    />
    
  </head>
  <body>
    <div class="label" style="font-style: italic;"  >
      <h1 style="font-size: 60px;"> <span class="label label-default"><a href="#" style="color:white" >Computer Networking And Management</span></a></h1>

    </div>
    <div class="container" >
        <div class="text-left " style=" margin-block-start: 50px; " >
          
            <form class="jumbotron" style="margin: 5px;">
              <p> 
                <div class="jumbotron" style="background-color:white;">
                      <h1> What Are Smart Objects?</h1>
                Jean-Philippe Vasseur, Adam Dunkels, in Interconnecting Smart Objects with IP, 2010
                
                1.1.7 Computer Networking
                Computer networking is about connecting computers to allow them to communicate with each other. Computers are connected using networks as shown in Figure 1.6. These networks were initially wired, but with the advent of mobile computing, wireless networks are available.
                
                
                Figure 1.6. Computer networking allows computers and systems to communicate with each other. It forms the basis of today’s Internet.
                
                The field of computer networking is significantly older than that of mobile computing. Computer networking began in the early 1960s when the breakthrough concepts of packet-switched networking were first described by Leonard Kleinrock at UCLA [151]. Earlier telephony networks were circuit-switched, and each connection (phone call) created a circuit through the network where all data were transported. With packet-switched networking, no circuits were constructed through the network. Instead, each message was transported as a packet through the network where each node would switch the packet depending on its destination address.
                
                After Kleinrock’s breakthrough, ARPANET was created as the first large-scale computer network built on the concepts of a packet-switched network. During the late 1970s and early 1980s, ARPANET was gradually replaced with the early versions of the Internet. ARPANET started to use the IP protocol suite in 1983 before becoming the Internet.
                
                The ARPANET and the Internet were built on a powerful concept called the end-to-end principle of system design, named by an influential paper by Jerome H. Saltzer, David P. Reed, and David D. Clark [218]. The end-to-end principle states that functionality in a system should be placed as long as possible toward the end points. For the Internet, this meant that the end systems, the computers that connected to the Internet, should perform most of the work in the communication over the network with the network acting relatively dumb. Thus the network would only provide a mechanism for sending packets to and from the end points. This principle has arguably been one of the most important aspects of the design of the Internet system, because it allowed the system to gracefully support an ever-growing flora of applications from simple e-mail and file transport of the 1980s through the Web revolution of the 1990s transmission to high-speed, real-time video, and audio transmissions of the 2000s. The end-to-end principle allowed the network to evolve separately from these applications, thus making it possible to support an ever-growing number of users and uses, without requiring complex re-engineering of the entire network and its protocols.
                
                The connection between computer networking and smart objects is evident: communication is one of the defining characteristics of smart objects. In this book, we argue that many of the concepts, protocols, and mechanisms that have been developed in the computer networking community are suitable for smart object networks.
                
                Computerization Movements and Tales of Technological Utopianism
                Suzanne Iacono, Rob Kling, in Computerization and Controversy (Second Edition), 1996
                </div>
                
                <div class="jumbotron" style="background-color:white;">
                <h1>Computer Networking</h1>
                The computer networking movement encompasses a wide range of domains, some of which overlap computer-based education. For example, this computer-based education movement currently advocates extensive network links across schools, libraries, art museums, and research centers. The computer networking CM pushes this vision even further, advocating the weaving together of all institutional sectors into one giant electronic web. This web is sometimes referred to as cyberspace, a term first used by Gibson (1984) in his science fiction novel, Neuromancer, to describe the electronic realm where the novel's action took place. Today, we use the term to refer to the place where all social interactions via computer networks occur.
                
                Prior to 1990, the physical manifestation of cyberspace was the Internet and the primary users were government employees, research scientists, and other academics in research universities and centers throughout the United States and western Europe. The Internet started as an ARPA demonstration project on internetworking in the early 1970s (Kahn, 1994). After splitting off from MILNET (the military network), the ARPA NET became known as the Internet. The National Science Foundation (NSF) paid for new computer science sites to be added to the Internet through CSNET (the Computer Science Network) and then commissioned the NSFNET to link NSF-funded supercomputer centers across the United States. The Department of Energy (DOE) and the National Aeronautics and Space Administration (NASA) built HEPNET (high-energy physics net), SPAN (space physics analysis net), ESNET (energy sciences net), and NSI (NASA science internet), all used primarily by research, academic, and government communities.
                
                In the last few years two related events have helped strengthen the public appeal of the Internet: the expansion and growing commercialization of the Internet and the promotion of technological support for the NII by the Clinton administration. These actions have already expanded the range of Internet services to most universities and colleges, many libraries, and some elementary and secondary schools. Government subsidy and other resources generated by CMOs along with strong advocacy about the transformational capacity of computerization by the media and popular magazines help to fuel the increasing demands for computer networking.
                
                While most users connect to the Internet through work or school, situations where the institution bears the financial burden, increasingly people are willing to pay for Internet connections out of their own pockets. Today, PCs can be found in over 26% of households in the United States and many of them are connected to local computer bulletin boards, online services, or the Internet (Brody, 1992). In 1992, New York City had two providers of public Internet access. In 1993, there were seven. More people have been connected to the Internet in the last two years than in the previous twenty. It is currently estimated that approximately four million homes in the United States are now connected to some type of online service and these numbers are growing daily (Eng and Lewyn, 1994). Current scenarios of home use portray exciting social interactions with distant people and places, primarily in the guise of entertainment, home shopping, and group game playing. The NII: Agenda for Action (the White House, 1993) states, “You could see the hottest video games, or bank and shop from the comfort of your home whenever you chose.” Although the Clinton administration consistently advocates universal access and affordable prices for users, today only the rich can afford the equipment and network connections required to be online.
                
                A less entertaining but nonetheless Utopian vision of computer networking focuses on uses that empower and preserve the public interest. Several networking CMOs, like the Society for Electronic Access (SEA), and the Center for Civic Networking (CCN) have focused attention on grass roots networks and the recreation of civic life. The first civic network was the Community Memory in Berkeley, California, started in the mid-1970s to strengthen and revitalize the Berkeley community (Schuler, 1994). Today, over one hundred civic networks are planned or are currently in operation in the United States. They include the Cleveland Free-Net (Ohio), Big Sky Telegraph (Montana), Electronic Cafe International (Santa Monica, CA), and the Cambridge (MA) Civic Forum, all based in and run by local communities in partnership with networking CMOs. Global civil networks have also emerged. Examples include PeaceNet, EcoNet, GreenNet, and ConflictNet, all of which are dedicated to peace, human rights, and environmental preservation. In 1990, these networks with the support of the MacArthur, Ford, and General Service foundations and the United Nations Development Program established the Association for Progressive Communications (APC) with partners in ten countries and affiliated systems in many other countries (Frederick, 1993).
                
                The goal of participants in these civic networks is to make information flows more democratic, break down power hierarchies, and circumvent information monopolies. At the local level, city- and region-wide citizen dialogue is considered critical to the development of innovative solutions for the improvement of government services, industrial competitiveness, and a revitalized democracy. At the global level, network enthusiasts argue that the present flow of world news is too regulated. Five news agencies around the world control about 96% of the world's news flows (Mowlana, 1986). By providing low-cost appropriate solutions, APC networks can democratize cyberspace and provide an effective counter-balance to trends in corporate control of the world's information flows.
                
                Computer networks are central to Utopian accounts of the next wave of human culture where much of life is spent online. Participants are referred to as settlers or homesteaders (Rheingold, 1993). Specialists are portrayed as cowboys with keyboards rather than six-guns. Exciting images of life at the frontier propel many into participation. Networking activists imply that there are no limits to what can be done in cyberspace by downplaying the actual costs of new technologies and the continuing benefits of physical forms of social interaction. Other media for learning, socializing, working, or revitalizing the community are treated as less important. Real life is life online. The physical world is relegated to IRL (in real life) or life off-line (Rheingold, 1993).
                
                The beliefs and strategies advocated by the two CMs have changed over the past several decades. But both have moved in the direction of increasing computerization and networked forms of social arrangements. Helping to fuel this momentum are Utopian visions that downplay the actual social choices that can constrain or inhibit institutions from making such large-scale changes and the political challenges that will certainly accompany them. These CMs require enormous resources and their orientation is sufficiently elitist that one might expect some systematic progressive alternative to it. In the next section, we focus on some organizations that have emerged to serve the public interest and which participate in the forming of national policy about computerization. We also discuss the challenges associated with the development of a general movement to counter computerization.
                
                Understanding Networks and Networked Video
                Anthony C. Caputo, in Digital Video Surveillance and Security (Second Edition), 2014
                 </div>
                <div class="jumbotron" style="background-color:white;"><h1>Getting Wired
                </h1>
                The Open System Interconnection (OSI) Reference Model (see Figure 4.1) of the International Standards Organization (ISO) is a primary architectural model for communications between computer systems. Many of the components of the OSI model exist in all communications systems. OSI defines the typical networking system architecture in seven layers: Application, Presentation, Session, Transport, Network, Data Link, and Physical layers, each with its own unique functions and protocols.
                
                
                FIGURE 4.1. OSI Reference Model layers.
                
                A computer networking protocol is the set of standards and rules that hardware and software must follow in order to communicate with all other computers. Transport Control Protocol/Internet Protocol (TCP/IP) has become a standard for computer communications. Any device running TCP/IP can communicate with another shared device anywhere in the world, running any type of operating system, software, or network topology, as long as it too is running TCP/IP. The four general layers that make up this “translator” TCP/IP (or Department of Defense, or DOD) model are:
                
                •
                Process and Application layer
                
                •
                Host-to-Host Transport layer
                
                •
                Internet layer
                
                •
                Network Access layer
                
                The Application layer holds the actual interface to the software and utilities that allow such networking functions as file and print sharing and authentication (e.g., HTTP, FTP). The Application layer passes the newly created data to the Transport layer. This layer consists of the Transfer Control Protocol (TCP) and the User Datagram Protocol (UDP) and is all about traveling—moving from one place to another.
                
                The Internet layer includes the Internet Protocol (IP), Address Resolution Protocol (ARP), and Internet Control Message Protocol (ICMP). IP addressing standardizes the way computers communicate with each other. This core scheme is responsible for the ubiquitous communications between devices, which has made convergence that much easier.
                
                An IP address contains four octets of decimal numbers that are derived from binary logic. IP is simply the protocol used to fragment packets and provide logical communication. For example, Domain Name Server (DNS) on the Application layer will resolve 192.168.0.1 to www.websitename.com. ARP will then resolve the IP address to a MAC address on the Data Link layer (or Network Access on the DOD), and Reverse ARP (RARP) will do the reverse.
                
                The lowest general layer is the Interface (Physical) layer, which consists of the data link and hardware used for connectivity. This is the layer where Ethernet joins the equation and the data link. Ethernet is possible with the inclusion of specific hardware, which is the physical hardware component of the layer. The Interface layer also provides the error-checking and correcting functions: checking for errors inside incoming data or packets, adding error-checking information to the outgoing data, and resending the data if there’s no returned acknowledgment of delivery.
                
                The Network Interface Card, sometimes referred to as a NIC (pronounced NICK) or network adapter, is another physical component of this layer. Along with its unique physical address or Media Access Control (MAC) address, it provides a unique signature on every network adapter, thus giving each computer a unique identification. The remaining majority of this layer is invisible, which greatly reduces its complexity. The physical MAC address of a computer’s network adapter can be found using the ipconfig/all command at the command prompt: Start > Run > type CMD and press Enter. Once the command prompt appears, type ipconfig/all (see Figure 4.2) and press Enter. The MAC address is called the physical address and is a series of double-digit octets separated by colons.
                
                
                FIGURE 4.2. To find your MAC address, at the command prompt type ipconfig/all and press Enter.
                     </div>
                <div class="jumbotron" style="background-color:white;"><h>Why Ethernet?</h>
                There are a few reasons that the Ethernet has become the networking juggernaut. First and foremost is that Ethernet equipment can be very cost-effective. The second reason is that Ethernet is infinitely extensible, managing the ever-demanding traffic of data, voice, video, and storage. Ethernet innovates to more easily evolve with its ever-growing bandwidth demands, beginning at 10 Mbps to 100 Mbps and onward to 1000 Mbps, 10 Gbps and, 40 Gbps, and even 100 Gbps Ethernet.
                
                The third reason is familiarity. Even when companies or associations introduce new, exciting networking technologies, Ethernet is already a comprehensive solution that is understood and actually works. The age-old familiarity with unshielded twisted pair (UTP) has made Ethernet an easy and ubiquitous solution. Just give a few Ethernet cables, a US$30 switch, and a few client stations to a 10-year-old who wants to do gaming on a network and you’ll have a functioning network in less time than it’ll take to boot up Windows.
                
                Only a few Ethernet topologies are being used today. The original bus topology used a straight daisy-chain of computers in a linear fashion. This setup limited the amount of distance and the maximum number of nodes (see Table 4.2). Even the type of coaxial cable used can limit the distance in a linear bus architecture, and because there are no hubs, switches, or routers, terminators were required at each end of the daisy chain.
                
                Table 4.2. General Network Designs
                
                Topology	Ethernet	Description	Cable Type	Maximum Cable Length	Maximum Nodes	Data Rate
                Bus (linear)	Yes	Straight daisy-chain connectivity with no hubs or routers but requires terminators at each end	RG8 coaxial (BNC) cable with terminators	500 meters	30	10 Mbps
                Bus (linear)	Yes	Straight daisy-chain connectivity with no hubs or routers but requires terminators at each end	RG58 coaxial (BNC) cable with terminators	185 meters	30	10 Mbps
                Token Ring	No	A constant messaging around a ring to avoid data collision	Category 2 or 4	45 meters	72	4 or 16 Mbps
                Star	Yes	Multiple workstations connected to a hub or switch	Category 5, 5e, or 6	100 meters	1024	10/100/1000 Mbps
                Tree	Yes	Multiple interconnected star networks	Category 5, 5e, or 6	100 meters	1024	10/100/1000 Mbps
                The once-popular Token Ring topology was not Ethernet, but it created constant messaging around the ring to avoid collision. Token Ring used coaxial, category 2 or 4 wire with up to 72 nodes (more than double that of a bus topology). However, the ring included a maximum distance of only 45 meters, and while the bus network offered up to 10 Mbps bandwidth, the Token Ring varied between 4 or 16 Mbps, depending on distance, cabling, and architecture.
                
                The star and tree topologies are true Ethernet architectures, providing speeds of up to 1 Gbps (more with the upcoming 10 Gbps Ethernet) and up to 1024 nodes. Although category 5, 5e, or 6 cable is the standard cabling with the RJ45 connector for Ethernet and it is limited to 100 meters, that 100 meters is between each network device. For example, a computer can be 1,000 meters away from a network router as long as there are two intermediate network switches at 100-meter intervals.
                
                If you are using Windows as your workstation operating system, you can create a simple peer-to-peer (serverless) network by installing network adapters into a free PCI slot on each of two computers (if not already built-in to the motherboard) and installing the appropriate software driver. Depending on the age of the NIC, the drivers may already be part of the Windows operating system (OS) or may come with the network adapter on a CD-ROM. You can connect the two computers by plugging a Category-5 crossover cable into the RJ45 Ethernet port on each network adapter. The two computers can now communicate using the TCP/IP built into Windows by way of the network adapter. The same holds true when you plug the same Cat5 cable from a computer to an IP camera or encoder, both of which include an embedded OS that also uses TCP/IP.
                
                A crossover cable is used to interconnect two computers by “crossing over” their respective pin connectors. Once the cables are plugged in, the two computers start communicating with each other using TCP/IP, and you have successfully created a simple peer-to-peer network (see Figure 4.3).
                
                
                FIGURE 4.3. A simple peer-to-peer network: Connect the Ethernet ports together with a crossover cable.
                
                Peer-to-Peer and Client/Server
                Peer-to-peer and client/server are two basic network types. A peer-to-peer network interconnects independent workstations to share files and printers (see Figure 4.4). Creating a peer-to-peer network is an easy installation but one that drains resources from each individual workstation, thus slowing their performance, and is less secure than a server-based network.
                
                
                FIGURE 4.4. Basic network sharing.
                
                The client/server environment provides a multitude of advantages touched on throughout this book, including:
                
                •
                Better security
                
                •
                Centralized administration
                
                •
                Reduction of processing burdens on individual workstations
                
                •
                Increased collaboration
                
                •
                More extendibility
                
                •
                Better maintainability
                
                The first step in network design involves creating a plan for configuring and arranging your network, known as a topology (see Figure 4.5). A topology is a pictorial description of how the network is set up, driven mostly by the office environment and locations. There are three basic types of topology: bus, token ring, and star. The bus topology is a daisy-chain linkage with termination and ground required on both ends. The token ring network incorporates nodes within a circular channel that is always “on” and looking for the right data. The star topology is the most popular because it gives you the option of linking as many as 1024 nodes to one network, whereas the bus network holds a maximum of 30 nodes and the token ring holds up to 72 nodes.
                
                
                FIGURE 4.5. Small to medium-sized company network example.
                  </div> 
                <div class="jumbotron" style="background-color:white;"> <h1>Maintaining Access</h1>
                James Broad, Andrew Bindner, in Hacking with Kali, 2014
                
                Introduction
                Exploiting a computer, networking device, or web service is great; however, the goal of most penetration tests is to maintain access to the compromised system. There are a number of methodologies for maintaining access to exploited victim systems; however, the overarching conclusion of every methodology is not to steal information but to reduce the time-consuming and exhaustive efforts required to keep attacking the same machine over and over after it’s already been compromised. If a security tester is working with a team, remote collocated servers or is in need of a secondary access point for a later access to the computer system, then efforts and expectation can be easily managed and further attacks can be more precise.
                
                Maintaining access is a secondary art form that involves just as much, if not more, thought than the exploitation of a system. This chapter covers the basic concepts of security testers and hackers alike use to maintain access and keep the compromised session going. Some of the concepts presented are very advanced. The reader should not get discouraged if reading this chapter doesn’t make sense the first time though. This chapter ends with a section designed to keep the reader’s attention focused and help reenforce the advanced methodologies presented.
                
            
                
                Performance Monitoring and Measurement
                Vinod Joseph, Brett Chapman, in Deploying QoS for Cisco IP and Next Generation Networks, 2009
                 </div>
                <div class="jumbotron" style="background-color:white;"><h1>Abstract Syntax Notation One</h1>
                In telecommunications and computer networking, Abstract Syntax Notation One (ASN.1) is a standard and flexible notation that describes data structures for representing, encoding, transmitting, and decoding data. It provides a set of formal rules for describing the structure of objects that are independent of machine-specific encoding techniques and is a precise, formal notation that removes ambiguities. This gives a well-known reference for interoperation between vendors where required.
                
                ASN.1 is a joint ISO and ITU-T standard, originally defined in 1984 as part of CCITT X.409:1984. ASN.1 moved to its own standard, X.208, in 1988 due to wide applicability. The substantially revised 1995 version is covered by the X.680 series. An adapted subset of ASN.1, Structure of Management Information (SMI), is specified in SNMP to define sets of related MIB objects; these sets are termed MIB modules.
                
                Introduction
                Paul Goransson, Chuck Black, in Software Defined Networks, 2014
                 </div>
                <div class="jumbotron" style="background-color:white;"><h1>The Modern Data Center</h1> 
                The Internet came to dominate computer networking to a degree rarely seen in the history of technology and was, at its core, connectionless and distributed. During this period of ascendance, the World Wide Web (WWW), an offspring of the Internet, spawned by Sir Tim Berners-Lee of the United Kingdom, gave rise to ever-growing data centers hosting ever more complex and more heavily subscribed web services. These data centers served as environmentally protected warehouses housing large numbers of computers that served as compute and storage servers. The warehouses themselves were protected against environmental entropy as much as possible by being situated in disaster-unlikely geographies with redundant power systems and ultimately with duplicate capacity at disparate geographical locations.
                
                Because of the large numbers of these servers, they were physically arranged into highly organized rows of racks of servers. Over time, the demand for efficiency drove the migration of individual server computers into server blades in densely packed racks. Racks of compute-servers were hierarchically organized such that top-of-rack (ToR) switches provided the networking within the rack and the inter-rack interface capability. We depict the rigid hierarchy typical in today’s data center in Figure 1.1.
                
                
                Figure 1.1. Typical data center network topology.
                
                During this evolution, the sheer numbers of computers in the data center grew rapidly. Data centers are being built now that will accommodate over 120,000 physical servers [8]. State-of-the-art physical servers can conceivably host 20 virtual machines (VMs) per physical server. This means that the internal network in such a data center would interconnect 2,400,000 hosts! These growing numbers of host computers within a data center all communicate with each other via a set of communications protocols and networking devices that were optimized to work over a large, disparate geographical area with unreliable communications links. Obviously, tens of thousands of computers sitting side by side in an environmental cocoon with highly reliable communications links is a different nature of network altogether. It gradually emerged that the goals of survivability in the face of lost communications links or a bombed-out network control center were not highly relevant in this emerging reality of the data center. Indeed, an enormous amount of complexity invented to survive precisely those situations was making untenable the operation of the ever larger and more complex data center.
                
                Beyond the obvious difference in the stability of the network topology, the sheer scale of these mega data centers in terms of nodes and links creates a network management challenge different from those encountered previously. Network management systems designed for carrier public networks or large corporate intranets simply cannot scale to these numbers. A new network management paradigm is needed.
                
                Furthermore, though these data centers exist in order to support interaction with the turbulent world outside their walls, current research [13] predicts that by 2014 80% of the traffic in data centers will be East-West traffic. East-West traffic is composed of packets sent by one host in a data center to another host in that same data center. Analogously, North-South traffic is traffic entering (leaving) the data center from (to) the outside world. For example, a user’s web browser query about the weather might be processed by a web server (North-South) that retrieves data from a storage server in the same data center (East-West) before responding to the user (North-South). The protocols designed to achieve robustness in the geographically dispersed wide-area Internet today require that routers spend more than 30% of their CPU cycles [8] rediscovering and recalculating routes for a network topology in the data center that is highly static and only changed under strict centralized control. This increasing preponderance of East-West traffic does not benefit from the overhead and complexities that have evolved in traditional network switches to provide just the decentralized survivability that Paul Baran so wisely envisioned for the WANs of the past.
                
                The mega data centers discussed in this section differ from prior networks in a number of ways: stability of topology, traffic patterns, and sheer scale. Traditional networking technologies are simply inadequate to scale to the levels being required. SDN, a technology designed explicitly to work well with this new breed of network, represents a fundamental transformation from traditional Internet switching. To understand how SDN does differ, in Sections 1.4 and 1.5 we review how legacy Internet switches work to establish a baseline for comparison with SDN.
                
                Before continuing, we should emphasize that although the modern data center is the premier driver behind the current SDN fervor, by no means is SDN applicable to only the data center. When we review SDN applications in Chapter 10 we will see that SDN technology can bring important innovations in domains that have little to do with the data center, such as mobility.
                
                Computer Networking for Education
                H. Mandl, F. Fischer, in International Encyclopedia of the Social & Behavioral Sciences, 2001
                
                5 Outlook
                A predominant trend in research on computer networking for education is to develop complex learning environments employing different tasks, as well as media mixes with multimedia, synchronous and asynchronous text-based communication or videoconferencing. With testbed designs or design experiments (Gomez et al. 1998), researchers try to design and evaluate those prototypical scenarios based on instructional theories and tools as well as on constraints of the content to be learned, learners' needs, and organizational limitations. We believe that this important evaluative and design-related research should be accompanied by more controlled experimental studies analyzing specific interactive and cognitive effects resulting from specific types of cooperative learning tasks supported by specific technologies.
                 </div>
                <div class="jumbotron" style="background-color:white;">
                    <h1>Troubleshooting the Juniper Firewall</h1>
                Brad Woodberg, ... Ralph Bonnell, in Configuring Juniper Networks NetScreen & SSG Firewalls, 2007
                
                Introduction
                Troubleshooting is a fact of life in computer networking, so this chapter covers the different ways to track the status of packets going through your firewall. Juniper firewalls offer a selection of tools to assist with troubleshooting network access. If you're already familiar with the troubleshooting tools available on the Juniper firewalls, you won't find many surprises when working with the new SSG product line since new features in the SSG firewall have similar troubleshooting tools, as well as a similar structure, to their NetScreen predecessors.
                
                When dealing with network firewalls, it's important to remember that they often change the content of the packets going through them. So it's our task to keep track of the changes and make sure they are what we intended. Most firewalls have four main functions: packet forwarding, stateful filtering, address translation, and encryption. We tackle each of these functions differently. For instance, troubleshooting packet forwarding can be as easy as inspecting the routing table. Address translation may require looking at a log of the traffic. Troubleshooting encryption may require analysis of a detailed packet dump. Juniper firewalls, on the other hand, offer specific troubleshooting tools built in to the ScreenOS operating system. Here, we cover the different troubleshooting facilities—from ping to firewall debug commands—to help you understand the full arsenal of troubleshooting capabilities the Juniper firewall provides you.
                
                Remember that every firewall issue is resolvable. There is a reason behind every decision the firewall makes. Thus, we begin this chapter by going through a common troubleshooting methodology which provides a solid process to help solve problems in an efficient manner. Next, we cover the various processes affecting a packet as it makes its way through the firewall. Then, we go over the different tools available for troubleshooting, along with the various firewall commands pertinent to troubleshooting. Following this introduction to troubleshooting commands, we discuss troubleshooting methods for VPNs (virtual private networks), NSRP (NetScreen Redundancy Protocol), and traffic shaping. Finally, we explore the logs the firewall creates in order to help us determine what the firewall is doing with our packets.
                
                Why SDN?
                Paul Göransson, ... Timothy Culver, in Software Defined Networks (Second Edition), 2017
                 </div> 
                <div class="jumbotron" style="background-color:white;">
                    <h1>Evolution of Switches and Control Planes</h1>
                We begin with a brief review of the evolution of switches and control planes that has culminated in a fertile playing field for SDN. This complements the material presented in Sections 1.4 and 1.5. The reader may find it useful to employ Fig. 2.1 as a visual guide through the following sections, as it provides a graphical summary of this evolution and allows the reader to understand the approximate timeframes when different components of switching moved from software to hardware.
                
                
                Fig. 2.1. Networking functionality migrating to hardware.
                  </div>
    
                <div class="jumbotron" style="background-color:white;">
                     <h1> Independence and Autonomy in Early Devices</h1>
                Early network device developers and standards-creators wanted each device to perform in an autonomous and independent manner, to the greatest extent possible. This was because networks were generally small and fixed, with large shared domains. A goal also was to simplify rudimentary management tasks and to make the networks as plug and playas possible. Their relatively static configuration needs were performed manually. Developers went to great lengths to implement this distributed environment with intelligence resident in every device. Whenever coordination between devices was required, collective decisions could be made through the collaborative exchange of information between devices.
                
                Interestingly, many of the goals of this distributed model, such as simplicity, ease-of-use, and automatic recovery, are similar to the goals of SDN, but as the scale and complexity of networks grew, the current distributed model has become increasingly dysfunctional.
                
                Examples of this distributed intelligence are the layer two (bridging) and layer three (routing) protocols, which involved negotiation between the devices in order to reach a consensus of how forwarding and routing would be performed. We introduced these protocols in Chapter 1 and provide more details on them below.
                
                •
                Spanning Tree Protocol
                
                Basic layer two forwarding, also known as transparent bridging, can be performed independently by each switch in the network. However, certain topologies require an imposition of a hierarchy on the network in order to prevent loops which would cause broadcast radiation. The Spanning Tree Protocol(STP) is an example of the operation of autonomous devices participating in a distributed decision-making process in order to create and enforce a hierarchy on the network. The result is the correct operation of transparent bridging throughout the domain, at the expense of convergence latency and possibly arbitrary configuration. This solution was a trade-off between cost and complexity. Multiple paths could have been supported but at greater cost. While STP was adequate when networks were of smaller scale, as networks grew the spanning tree solution has become problematic. These problems manifest themselves in a striking fashion when networks reach the scale of the modern data center. For example, IEEE 802.1D specifies the following default timers for STP: fifteen seconds for listening, fifteen seconds for learning, and twenty seconds for max-age timeout. In older networks, convergence times of thirty to fifty seconds were common. Such delays are not acceptable in today’s data centers. As the scale of the layer two network grows, the likelihood of greater delays increases. The Rapid Spanning Tree Protocol(RSTP) protocol, specified in IEEE 802.1D-2004 [1], improves this latency significantly but unfortunately is not deployed in many environments.
                
                •
                Shortest Path Bridging
                
                STP allowed only one active path to a destination, suffered from relatively slow convergence times, and was restricted to small network topologies. While the newer implementations of STP have improved the convergence times, the single active path shortcoming has been addressed in a new layer two protocol, SPB, introduced in Section 1.5.1. SPB is a mechanism for allowing multiple concurrent paths through a layer two fabric through collaborative and distributed calculation of shortest and most efficient paths, and sharing that information amongst the participating nodes in the meshed network. This characteristic is called multipath. SPB accomplishes this by utilizing IS-IS to construct a graph representing the layer two link-state topology. Once this graph exists, shortest path calculations are straightforward, though more complex than with spanning tree.
                
                To elaborate on what we mean by shortest path calculations, in Fig. 2.2 we depict a simple graph that can be used for calculating shortest paths in a network with six switches. The costs assigned to the various links may be assigned their values according to different criteria. A simple criterion is to make the cost of a network link inversely proportional to its bandwidth. Thus, the cost of transiting a ten Gbps link is one-tenth that of transiting a one Gbps link. When the shortest path calculation is complete, the node performing the calculation knows the least-cost path to any of the other nodes in the network. The least-cost path is considered the shortest path. For the sake of clarity, we should point out that IS-IS is used in the SPB context strictly for layer two path calculation. This differs from its classical application in calculating layer three routes, as described below. In the trivial example of Fig. 2.2, there is a single shortest path from node A to every other node. In real life networks it is common for there to be more than one least cost path between two nodes. The multipath characteristic of SPB would allow the traffic to be distributed across those multiple paths.
                
                
                Fig. 2.2. Example of graph of network for shortest path calculation.
                
                •
                RIP, BGP, OSPF, and IS-IS
                
                Routing at layer three requires cooperation between devices in order to know which routers are attaching which subnets to the network. In Chapter 1 we provided background on four routing protocols: RIP, BGP, OSPF, and IS-IS. These routing protocols involve the sharing of local routing information by each device, either at the edge of the network or as an intermediate node. Their collective sharing of information allows the routing state to converge as devices share their information with each other. Each router remains autonomous in terms of its ability to make routing decisions as packets arrive. This process is one of peers sharing and negotiating amongst themselves, without a centralized entity aiding in the decision.
                   </div>
                <div class="jumbotron" style="background-color:white;">
                <h1> Software Moves Into Silicon</h1>
                Vendors originally had to write their own software to implement even the basic functions like layer two forwarding and routing. Fig. 2.1 shows that over time these more basic functions moved from software into hardware. We now see most forwarding and filtering decisions implemented entirely in hardware. These decisions are driven by configured tables, set by the control plane software above. This shift of the lower-level decision-making from software to hardware has yielded tremendous improvements in the performance/cost ratio of switching equipment.
                
                Today, switching devices are typically composed of hardware components such as Application-Specific Integrated Circuits(ASICs), Field-Programmable Gate Arrays(FPGAs), and Ternary Content-Addressable Memories(TCAMs). The combined power of these integrated circuits allows for the forwarding decisions to be made entirely in the hardware at line rate. This has become more critical as network speeds have increased from one Gbps to ten Gbps, to forty Gbps, and beyond. The hardware is now capable of handling all forwarding, routing, Access Control List(ACL), and QoS decisions. Higher-level control functions, responsible for network-wide collaboration with other devices, are implemented in software. This control software runs independently in each network device.
                
                <div class="jumbotron" style="background-color:white;">
                <h1> Hardware Forwarding and Control in Software</h1>
                The network device evolution we have recounted thus far has yielded the following current situation:
                
                •
                Bridging (Layer Two Forwarding)
                
                Basic layer two MAC forwarding of packets is handled in the hardware tables.
                
                •
                Routing (Layer Three Forwarding)
                
                In order to keep up with today’s high-speed links and to route packets at link speeds, layer three forwarding functionality is also implemented in hardware tables.
                
                •
                Advanced Filtering and Prioritization
                
                General traffic management rules, such as ACLs, which filter, forward, and prioritize packets, are handled via hardware tables located in the hardware (e.g., in TCAMs), and accessed through low-level software.
                
                •
                Control
                
                The control software used to make broader routing decisions and to interact with other devices in order to converge on topologies and routing paths is implemented in software that runs autonomously inside the devices. Since the current control plane software in networking devices lacks the ability to distribute policy information about such things as security, QoS and ACLs, these features must still be provisioned through relatively primitive configuration and management interfaces.
                
                Given this landscape of (1) layer two and layer three hardware handling most forwarding tasks, (2) software in the device providing control plane functionality, and (3) policy implemented via configuration and management interfaces, an opportunity presents itself to simplify networking devices and move forward to the next generation of networking.
                 </div>
                <div class="jumbotron" style="background-color:white;">
                <h1> The Growing Need for Simplification </h1>
                In [2] the authors state that one of the major drivers for SDN is simplification. As time has passed, networking devices have become increasingly more complex. This is due in part to the existing independent and autonomous design of devices that make it necessary that so much intelligence be placed inside each device. Placing more functionality in hardware in some ways simplifies the device, but in other ways makes it more complicated because of the difficult handshakes and tradeoffs between handling packets in hardware versus software.
                
                Attempting to provide simplicity by adding features to legacy devices tends to complicate implementations rather than simplifying them. An analogy to the evolution of the Central Processing Unit(CPU) can be made here. Over time CPUs became highly complex as they attempted to support more and more functions. Ultimately, another simpler, easier to use CPU model emerged which was called the Reduced Instruction Set Computing(RISC) model. In the same way the RISC architecture served as a reset to CPU architecture, so, too, SDN may serve as a simplifying reset for network equipment design.
                
                In addition to simplifying the devices themselves, there is an opportunity to simplify the management of the networks of these devices. Rather than using primitive network management tools such as SNMP and CLI, network operators would prefer to use policy-based management systems. SDN may enable such solutions [3].
                
                 </div>
                
                <div class="jumbotron" style="background-color:white;">
                <h1>Moving Control Off of the Device</h1>
                <p>We remind the reader that control software in our context is the intelligence that determines optimal paths and responds to outages and new networking demands. At its core, SDN is about moving that control software off of the device and into a centrally located compute resource which is capable of seeing the entire network and making decisions which are optimal, given a complete understanding of the situation. While we will discuss this in much greater detail in the chapters that follow, basically, SDN attempts to segregate network activities in the following manner:

                </p>
                
                <ol>Forwarding, Filtering, and Prioritization
                
                    Forwarding responsibilities, implemented in hardware tables, remain on the device. In addition, features such as filtering based on ACLs and traffic prioritization, are enforced locally on the device as well.
                </ol>
            
                
               <ol>Control
                
                        Complicated control software is removed from the device and placed into a centralized controller, which has a complete view of the network and the ability to make optimal forwarding and routing decisions. There is a migration to a programming paradigm for the control plane. The basic forwarding hardware on the networking device is available to be programmed by external software on the controller. The control plane is no longer embedded, closed, closely coupled with the hardware, or optimized for particular embedded environments. 
                
                
               </ol>
              <ol> Application
                
                Above the controller is where the network applications run, implementing higher-level functions and, additionally, participating in decisions about how best to manage and control packet forwarding and distribution within the network.
                
                Subsequent chapters will examine in greater detail how this can be achieved, with a minimum of investment and change by networking vendors, while providing the maximum control and capability by the controller and its applications. The next section of this chapter will discuss another major reason why SDN is needed today—the cost of networking devices.
              </ol>
              </p>
            </div>
              
          </form>
      </div>
    </div>
  </body>
</html>
